# ğŸš€ Production-Ready LLM API with FastAPI & TinyLlama

This project is a **production-ready Large Language Model (LLM) service** built using **FastAPI** and powered by **TinyLlama**. It exposes a scalable, low-latency REST API for text generation and can be easily deployed in cloud or on-prem environments.

---

## âœ¨ Features

* âš¡ **FastAPI-based REST API** for high-performance inference
* ğŸ§  **TinyLlama** for lightweight, efficient LLM inference
* ğŸ­ **Production-ready architecture** (modular, scalable, maintainable)
* ğŸ”’ Input validation & error handling
* ğŸ“ˆ Easy to integrate with frontend, microservices, or agents
* ğŸ³ Docker-ready for containerized deployment
* ğŸ” Supports batch and single-prompt inference


## ğŸ§  Model Details

* **Model**: TinyLlama
* **Use case**: Lightweight text generation, chat, and prompt-based inference
* **Why TinyLlama?**

  * Low memory footprint
  * Fast inference
  * Ideal for edge devices and cost-efficient deployments

---

## ğŸš¦ API Endpoints

### ğŸ”¹ Generate Text

**POST** `/generate`

**Request Body**

```json
{
  "prompt": "Explain transformers in simple terms",
  "max_tokens": 150,
  "temperature": 0.7
}
```

**Response**

```json
{
  "response": "Transformers are neural network models that..."
}
```

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/llm-fastapi-tinyllama.git
cd llm-fastapi-tinyllama
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Running the Application

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

Visit API docs:

* Swagger UI â†’ `http://localhost:8000/docs`
* ReDoc â†’ `http://localhost:8000/redoc`

---

## ğŸ³ Docker Deployment

### Build Image

```bash
docker build -t tinyllama-api .
```

### Run Container

```bash
docker run -p 8000:8000 tinyllama-api
```

---

## ğŸ“ˆ Production Considerations

* âœ… Model loaded once at startup (no reload per request)
* âœ… Async FastAPI endpoints
* âœ… Configurable inference parameters
* âœ… Ready for:

  * Kubernetes
  * AWS ECS / EKS
  * Azure Container Apps
* ğŸ”„ Can be extended with:

  * Redis caching
  * Request queuing
  * Authentication (JWT / API keys)
  * Observability (Prometheus, OpenTelemetry)

---

## ğŸ§ª Testing

```bash
pytest tests/
```

---

## ğŸ” Security Notes

* Validate and sanitize user inputs
* Limit max tokens to prevent abuse
* Add rate limiting for public APIs
* Use HTTPS in production

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **FastAPI**
* **Pydantic**
* **TinyLlama**
* **Uvicorn**
* **Docker**

